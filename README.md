# ğŸ“Š MoE MemoryGraph: Enterprise Customer Intelligence System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue?style=flat&logo=python&logoColor=white)
![LangGraph](https://img.shields.io/badge/LangGraph-Latest-green?style=flat)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16+-336791?style=flat&logo=postgresql&logoColor=white)
![pgvector](https://img.shields.io/badge/pgvector-0.5+-orange?style=flat)
![License](https://img.shields.io/badge/License-MIT-yellow?style=flat)
![Status](https://img.shields.io/badge/Status-Active-success?style=flat)

</div>

---

A production-grade Mixture-of-Experts (MoE) agentic system for intelligent customer support analysis. Combines parallel expert execution with a 4-plane memory architecture (STM/LTM/Semantic/Vector) backed by PostgreSQL + pgvector to answer strategic business questions in secondsâ€”with full citations, confidence scores, and audit trails.

---

## ğŸ¯ Problem & Solution

### The Problem
Customer success teams waste **5+ hours** manually searching thousands of support tickets to answer strategic questions:
- *"Why did enterprise customers churn in Q4?"*
- *"What compliance risks exist in our support conversations?"*
- *"Generate a health report for Acme Corp with evidence"*

**Current approach:** Read 100+ tickets manually â†’ Check CRM notes â†’ Ask around â†’ Write report based on memory  
**Result:** Slow, low confidence, no proof, misses patterns

### The Solution
An intelligent memory system that:
- âœ… Answers complex queries in **30 seconds** with citations
- âœ… Routes queries through specialized experts (semantic/vector/LTM/compliance)
- âœ… Provides **confidence scores** and full **audit trails**
- âœ… Learns from usage patterns via write-back loops

---

## âœ¨ Key Features

### ğŸ§  **Four-Plane Memory Architecture**
| Plane | Purpose | Example |
|-------|---------|---------|
| **Semantic** | Structured facts with provenance | `Acme Corp â†’ status â†’ churned (confidence: 0.95)` |
| **Vector** | pgvector similarity search | "Find tickets similar to 'billing issue'" |
| **LTM** | Distilled patterns/trends | "Pricing complaints up 40% in Q4" |
| **STM** | Session context (TTL) | "User asked about pricing 2 mins ago" |

### âš¡ **Explainable MoE Gating**
Smart routing selects which experts to run based on query complexity:

```python
Query: "Why did Acme Corp churn?"

Gate Decision:
  âœ“ SemanticQuery    (weight: 0.9) - 3 facts found
  âœ“ LTMRecall        (weight: 0.8) - pattern available  
  âœ— VectorRetrieve   (skipped)     - semantic coverage sufficient
  âœ“ Compliance       (weight: 1.0) - always run

Rationale: "High semantic confidence (0.92), skipping expensive vector search"
```

### ğŸš€ **Production-Grade Concurrency**
- **Batch parallelism**: Process 50+ queries simultaneously
- **Expert fan-out**: Run 5 experts in parallel per query (`asyncio.TaskGroup`)
- **Backpressure**: Semaphores + per-expert 5s timeouts
- **Resilience**: Exponential backoff retries for transient failures

### ğŸ” **Auditable Outputs**
Every answer includes full provenance:

```json
{
  "answer": "Acme Corp churned due to pricing concerns mentioned in 5 tickets...",
  "confidence": 0.89,
  "citations": [
    {"type": "semantic", "fact_id": "f042", "source": "ticket_#1201"},
    {"type": "ltm", "summary_id": "l005", "text": "Q4 pricing complaints +40%"}
  ],
  "audit": {
    "gate_rationale": "High semantic coverage, skipped vector search",
    "experts_run": ["SemanticQuery", "LTMRecall", "Compliance"],
    "expert_scores": {"SemanticQuery": 0.92, "LTMRecall": 0.81},
    "pii_detected": false,
    "latency_ms": 847
  }
}
```

### ğŸ›¡ï¸ **Enterprise Controls**
- **Multi-tenant isolation**: Strict `tenant_id` filtering in all queries
- **PII detection**: Automatic redaction with toggleable policies
- **Write-back learning**: System distills patterns to LTM after queries
- **Observability**: Structured logs with `run_id`, `item_id` tracing (OpenTelemetry-ready)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query: "Why did enterprise customers churn in Q4?" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  MoE Gate      â”‚  Decides which experts to run
         â”‚  - Semantic?   â”‚  based on memory availability
         â”‚  - Vector?     â”‚  and query complexity
         â”‚  - LTM?        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Semanticâ”‚   â”‚Vector â”‚   â”‚  LTM  â”‚  Run in parallel
â”‚ Facts  â”‚   â”‚Search â”‚   â”‚Recall â”‚  with asyncio
â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚            â”‚            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Aggregate +   â”‚  Weighted merge
         â”‚  Synthesize    â”‚  + conflict resolution
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  JSON Output   â”‚  Answer + citations
         â”‚  + Audit Trail â”‚  + confidence
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**LangGraph Workflow:**
1. **LoadMemoryPreview** â†’ Parallel fetch STM/LTM/Semantic/Vector previews
2. **MoEGate** â†’ Compute execution plan (experts + weights + rationale)
3. **RunExpertsParallel** â†’ Execute selected experts with timeouts
4. **Aggregate** â†’ Weighted merge + resolve conflicts
5. **Synthesize** â†’ Generate structured answer with citations
6. **WriteBack** â†’ Update STM, distill to LTM, commit verified facts

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Docker & Docker Compose
- [uv](https://github.com/astral-sh/uv) (recommended) or pip

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/moe-memorygraph.git
cd moe-memorygraph

# Install dependencies with uv
uv sync

# Or with pip
pip install -e .

# Start PostgreSQL + pgvector
docker-compose up -d

# Run migrations
uv run alembic upgrade head

# Load sample dataset (Bitext customer support)
uv run python -m moe_memorygraph.cli.ingest --limit 1000
```

### Run Your First Query

```bash
# Single query
uv run python -m moe_memorygraph.cli.run_item \
  "Why did Acme Corp churn?" \
  --tenant acme_corp

# Batch queries
uv run python -m moe_memorygraph.cli.run_batch queries.json
```

**Expected Output:**
```json
{
  "run_id": "run_abc123",
  "query": "Why did Acme Corp churn?",
  "answer": "Acme Corp churned on Dec 15, 2024 due to pricing concerns...",
  "confidence": 0.89,
  "citations": [...],
  "audit": {
    "experts_run": ["SemanticQuery", "LTMRecall"],
    "latency_ms": 847
  }
}
```

---

## ğŸ“‚ Project Structure

```
moe-memorygraph/
â”œâ”€â”€ src/moe_memorygraph/
â”‚   â”œâ”€â”€ core/           # Config, types, errors, logging
â”‚   â”œâ”€â”€ db/             # SQLAlchemy models + repositories
â”‚   â”œâ”€â”€ memory/         # 4-plane memory stores
â”‚   â”œâ”€â”€ experts/        # Expert implementations
â”‚   â”œâ”€â”€ gating/         # MoE routing logic
â”‚   â”œâ”€â”€ graph/          # LangGraph workflow
â”‚   â”œâ”€â”€ ingestion/      # Data pipeline (HuggingFace â†’ DB)
â”‚   â””â”€â”€ cli/            # Command-line interface
â”œâ”€â”€ migrations/         # Alembic database migrations
â”œâ”€â”€ tests/              # Unit + integration tests
â”œâ”€â”€ docs/               # Architecture & design docs
â””â”€â”€ docker-compose.yml  # Local Postgres + pgvector
```

---

## ğŸ“ Expert Types

| Expert | Purpose | Example Output |
|--------|---------|----------------|
| **STMRecall** | Fetch recent session context | "User asked about pricing 2 mins ago" |
| **LTMRecall** | Retrieve distilled patterns | "Acme complained about SSO 12x in Q4" |
| **SemanticQuery** | Query structured facts | `acme_corp â†’ status â†’ churned` |
| **VectorRetrieve** | Similarity search | Top-5 tickets similar to query |
| **Verifier** | Fact-check expert outputs | Confidence adjustment based on conflicts |
| **Compliance** | PII detection/redaction | Redact emails, phone numbers, names |
| **Synthesizer** | Generate final answer | Structured JSON with citations |

---

## ğŸ“Š Dataset

**Primary:** [Bitext Customer Support Dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset)  
- 27,000+ support conversations
- Categories: cancellations, billing, complaints, technical issues
- Human-curated with intent labels

**Supplementary:**
- [ai4privacy/pii-masking-300k](https://huggingface.co/datasets/ai4privacy/pii-masking-300k) (PII testing)
- [tau/soda](https://huggingface.co/datasets/tau/soda) (multi-turn conversations)

---

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest

# Unit tests only
uv run pytest tests/unit/

# Integration tests (requires DB)
docker-compose up -d
uv run pytest tests/integration/

# Evaluation harness (golden queries)
uv run python scripts/run_evaluation.py
```

**Test Coverage:**
- Gate routing logic (different experts for different queries)
- Memory store CRUD operations
- Expert execution with timeouts/retries
- Parallel batch processing
- PII detection accuracy

---

## ğŸ“ˆ Performance

**Benchmarks** (MacBook Pro M1, 16GB RAM):
- Single query: **~850ms** (5 experts in parallel)
- Batch (50 queries): **~28 seconds** (parallelism = 20)
- Memory operations: **<50ms** per store access
- Vector search: **<100ms** for 10k embeddings

**Scalability:**
- âœ… Tested with 10,000 vector chunks
- âœ… Tested with 5,000 semantic facts
- âœ… Handles 50 concurrent queries without backpressure
- âœ… Multi-tenant isolation verified across 3 tenants

---

## ğŸ› ï¸ Configuration

**Environment Variables** (`.env`):
```bash
# Database
DATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/memorygraph

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIM=384

# MoE Settings
MAX_CONCURRENT_EXPERTS=10
EXPERT_TIMEOUT_SEC=5
GATE_SEMANTIC_THRESHOLD=0.85

# Features
ENABLE_PII_DETECTION=true
ENABLE_WRITEBACK=true
```

---

## ğŸš¢ Deployment

### Docker
```bash
docker build -t moe-memorygraph .
docker run -e DATABASE_URL=... moe-memorygraph
```

### Kubernetes (Helm)
```bash
helm install moe-memorygraph ./infra/k8s/helm/
```

### Cloud Platforms
- **Railway**: One-click deploy with Postgres
- **Render**: Web service + managed database
- **AWS ECS**: Production-grade with RDS

---

## ğŸ”¬ Research & Extensions

**Potential enhancements:**
- [ ] Multi-modal memory (images, PDFs)
- [ ] Real-time streaming responses
- [ ] Adaptive gating (learn routing from feedback)
- [ ] Distributed expert execution (Celery/Ray)
- [ ] Graph-based semantic memory (Neo4j)

**Academic connections:**
- Mixture-of-Experts (MoE) routing
- Memory-augmented neural networks
- Retrieval-Augmented Generation (RAG)
- Multi-agent systems

---

## ğŸ“š Documentation

- [Architecture Deep Dive](docs/ARCHITECTURE.md)
- [Memory Model Design](docs/MEMORY_MODEL.md)
- [MoE Gating Logic](docs/MOE_GATING.md)
- [Deployment Runbook](docs/RUNBOOK.md)

---

## ğŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) first.

**Areas needing help:**
- Additional expert implementations
- Performance optimizations
- Documentation improvements
- Test coverage expansion

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **LangGraph** for orchestration framework
- **pgvector** for efficient vector search
- **Bitext** for customer support dataset
- **sentence-transformers** for embeddings

---

## ğŸ“ Contact

**Author:** Your Name  
**Email:** your.email@example.com  
**GitHub:** [@yourusername](https://github.com/yourusername)  
**LinkedIn:** [Your Profile](https://linkedin.com/in/yourprofile)

---

<div align="center">

**â­ Star this repo if you find it useful! â­**

Built with â¤ï¸ for production LLM systems

</div>
